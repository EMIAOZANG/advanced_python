{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA-3001 Advanced Python for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure you **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart). You can then run the cells **in order**, during the class.\n",
    "\n",
    "Any textual answers that need to be provided will be marked with \"YOUR ANSWER HERE\". Replace this text with your answer to the question.\n",
    "\n",
    "Any code answers that need to be provided will be marked with:\n",
    "\n",
    "```\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Replace all this code with your answer to the question. If you do not answer the question, the `NotImplementedError` exception will be raised, which will indicate to the grader that no answer has been supplied.\n",
    "\n",
    "In many cases, code answers will also have some associated test code. You should execute the tests after you have entered your code in order to ensure that your answer is correct. You should not proceed to the next question until your answer is correct.\n",
    "\n",
    "Finally, insert your Net ID and the Net ID's of any collaborators in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "NET_ID = \"jl6583\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data With PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any distributed computing framework needs to solve two problems: how to distribute data and how to distribute computation. Once such framework is [Apache Hadoop](http://hadoop.apache.org). Hadoop uses the Hadoop Distributed Filesystem (HDFS) to solve the distributed data problem and MapReduce as the programming paradigm that provides effective distributed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Spark](https://spark.apache.org) is a general purpose cluster computing framework that provides ***efficient in-memory computations for large data sets*** by distributing computation across multiple computers. Spark can utilize the Hadoop framework or run standalone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a functional programming API in multiple languages that provides more operators than map and reduce, and does this via a distributed data framework called ***resilient distributed datasets*** or ***RDDs***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are essentially a programming abstraction that represents a read-only collection of objects that are partitioned across machines. RDDs are fault tolerant and are accessed via parallel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because RDDs can be cached in memory, Spark is extremely effective at iterative applications, where the data is being reused throughout the course of an algorithm. Most machine learning and optimization algorithms are iterative, making Spark an extremely effective tool for data science. Additionally, because Spark is so fast, it can be accessed in an interactive fashion via a command line prompt similar to the Python REPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark library itself contains a lot of the application elements that have found their way into most Big Data applications including support for SQL-like querying of big data, machine learning and graph algorithms, and even support for live streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core components are:\n",
    "\n",
    "- ***Spark Core***: Contains the basic functionality of Spark; in particular the APIs that define RDDs and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.\n",
    "- ***Spark SQL***: Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive Query Language (HiveQL). Every database table is represented as an RDD and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.\n",
    "- ***Spark Streaming***: Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similar to how you would interact with a normal RDD as data is flowing in.\n",
    "- ***MLlib***: A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms like classifications, regressions, etc. that require iterative operations across large data sets. The Mahout library, formerly the Big Data machine learning library of choice, will move to Spark for its implementations in the future.\n",
    "- ***GraphX***: A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating subgraphs, or accessing all vertices in a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these components meet many Big Data requirements as well as the algorithmic and computational requirements of many data science tasks, Spark has been growing rapidly in popularity. Not only that, but Spark provides APIs in Scala, Java, and Python; meeting the needs for many different groups and allowing more data scientists to easily adopt Spark as their Big Data solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce primer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is a software framework for processing large data sets in a distributed fashion over a several machines.  The core idea behind MapReduce is mapping your data set into a collection of (key, value) pairs, and then reducing over all pairs with the same key. The overall concept is simple, but is actually quite expressive when you consider that: \n",
    " \n",
    "1. Almost all data can be mapped into (key, value) pairs somehow, and \n",
    "2. Your keys and values may be of any type: strings, integers, dummy types, and, of course, (key, value) pairs themselves\n",
    "\n",
    "The canonical MapReduce use case is counting word frequencies in a large text, but some other examples of what you can do in the MapReduce framework include: \n",
    " \n",
    "- Distributed sort \n",
    "- Distributed search \n",
    "- Web‐link graph traversal \n",
    "- Machine learning \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of occurrences of words in a text is sometimes considered as the “Hello world!” equivalent of MapReduce. A classical way to write such a program is presented in the python script below. The script is very simple. It parses the file from which it extracts and counts words and stores the result in a dictionary that uses words as keys and the number of occurrences as values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download [Moby Dick, the novel by Herman Melville](https://www.gutenberg.org/cache/epub/2701/pg2701.txt) and place the `pg2701.txt` file in the `/tmp` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: the = 14620\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# remove any non-words and split lines into separate words\n",
    "# finally, convert all words to lowercase\n",
    "def splitter(line):\n",
    "    line = re.sub(r'^\\W+|\\W+$', '', line)\n",
    "    return map(str.lower, re.split(r'\\W+', line))\n",
    "  \n",
    "sums = {}\n",
    "try:\n",
    "    in_file = open('/tmp/pg2701.txt', 'r')\n",
    "\n",
    "    for line in in_file:\n",
    "        for word in splitter(line):\n",
    "            word = word.lower()\n",
    "            sums[word] = sums.get(word, 0) + 1\n",
    "            \n",
    "    in_file.close()\n",
    "\n",
    "except IOError:\n",
    "    print \"error performing file operation\"\n",
    "else:\n",
    "    M = max(sums.iterkeys(), key=lambda k: sums[k])\n",
    "    print \"max: %s = %d\" % (M, sums[M])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with this approach comes from the fact that it requires the use of a dictionary, i.e., a central data structure used to progressively build and store all the intermediate results until the final result is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code we use can only run on a single processor, the best we can expect is that the **time necessary to process a given text will be proportional to the size of the text** (i.e., the number of words processed per second is constant). Actually, the performance degrades as the size of the dictionary grows. As shown on the diagram below, the number of words processed per second diminishes when the size of the dictionary reaches the size of the processor data cache (note that if the cache is structured in several layers of different speeds, the processing speed will decrease each time the dictionary reaches the size of a layer). A new diminution of the processing speed will be reached when the dictionary reaches the size of the Random Access Memory. Eventually, if the dictionary continues to grow, it will exceed the capacity of the swap and an exception will be raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgal8zM3R5Y204aDg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MapReduce aproach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of the MapReduce approach is that it does not require a central data structure. \n",
    "\n",
    "MapReduce consists of 3 steps:\n",
    "\n",
    "1. A ***mapping*** step that produces intermediate results and associates them with an output key;\n",
    "2. A ***shuffling*** step that groups intermediate results associated with the same output key; and\n",
    "3. A ***reducing*** step that processes groups of intermediate results with the same output key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cs.calvin.edu/courses/cs/374/exercises/12/lab/MapReduceWordCount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping step is very simple. The idea is to apply a function to each element of a list and collect the result. Python provides the `map` function that takes a function and sequence of input values and returns a sequence of values that have had the function applied to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our word count example, we want to map each word in the input file into a key/value pair containing the word as key and the number of occurances as the value. This is used to represent an intermediate result that says: “this word occurs one time”. This is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Deer', 1), ('Bear', 1), ('River', 1), ('Car', 1), ('Car', 1), ('River', 1), ('Deer', 1), ('Car', 1), ('Bear', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = ['Deer', 'Bear', 'River', 'Car', 'Car', 'River', 'Deer', 'Car', 'Bear']\n",
    "mapping = map((lambda x : (x, 1)), words)\n",
    "print mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using `map` for our example would result in reading the whole file into memory before we can perform the map operation. This would be no better than the original version, so instead it is done using a temporary file (that we will use later), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = '/tmp/pg2701.txt'\n",
    "map_file = '/tmp/pg2701.txt.map'\n",
    "sorted_map_file = '/tmp/pg2701.txt.map.sorted'\n",
    "\n",
    "\n",
    "# Implement our mapping function\n",
    "import re\n",
    "sums = {}\n",
    "try:\n",
    "    in_file = open(input_file, 'r')\n",
    "    out_file = open(map_file, 'w')\n",
    "\n",
    "    for line in in_file:\n",
    "        for word in splitter(line):\n",
    "            out_file.write(word.lower() + \"\\t1\\n\") # Separate key and value with 'tab'\n",
    "            \n",
    "    in_file.close()\n",
    "    out_file.close()\n",
    "\n",
    "except IOError:\n",
    "    print \"error performing file operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shuffling step consists of grouping all the intermediate values that have the same output key. In our word count example, we want to sort the intermediate key/value pairs on their keys. We can use the `sorted` function for the simple case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bear', 1), ('Bear', 1), ('Car', 1), ('Car', 1), ('Car', 1), ('Deer', 1), ('Deer', 1), ('River', 1), ('River', 1)]\n"
     ]
    }
   ],
   "source": [
    "sorted_mapping = sorted(mapping)\n",
    "print sorted_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is sill using the in-memory copy. Instead, we can use a Python program for sorting large files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_index(filename):\n",
    "    index = []\n",
    "    f = open(filename)\n",
    "    while True:\n",
    "        offset = f.tell()\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        length = len(line)\n",
    "        col = line.split('\\t')[0].strip()\n",
    "        index.append((col, offset, length))\n",
    "    f.close()\n",
    "    index.sort()\n",
    "    return index\n",
    "\n",
    "try:\n",
    "    index = build_index(map_file)\n",
    "    in_file = open(map_file, 'r')\n",
    "    out_file = open(sorted_map_file, 'w')\n",
    "    for col, offset, length in index:\n",
    "        in_file.seek(offset)\n",
    "        out_file.write(in_file.read(length))\n",
    "    in_file.close()\n",
    "    out_file.close()\n",
    "except IOError:\n",
    "    print \"error performing file operation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reduction step, we just need to count the number of values with the same key. Now that the different values are ordered by keys (i.e., the different words are listed in alphabetic order), it becomes easy to count the number of times they occur by summing values as long as they have the same key. Using lambda functions in Python, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bear', 2), ('Car', 3), ('Deer', 2), ('River', 2)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "# 1. Group by key yielding (key, grouper)\n",
    "# 2. For each pair, yield (key, reduce(func, last element of each grouper))\n",
    "grouper = groupby(sorted_mapping, lambda p:p[0])\n",
    "print map(lambda l: (l[0], reduce(lambda x, y: x + y, map(lambda p:p[1], l[1]))), grouper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sorted mapping file, it's also straight forward. We just read each key/value pair and continue to count until we find a different key. We just print out the value, then reset the values for the next key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: the = 14620\n"
     ]
    }
   ],
   "source": [
    "previous = None\n",
    "M = [None, 0]\n",
    "\n",
    "def checkmax(key, sum):\n",
    "    global m, M\n",
    "    if M[1] < sum:\n",
    "        M[1] = sum\n",
    "        M[0] = key\n",
    "\n",
    "try:\n",
    "    in_file = open(sorted_map_file, 'r')\n",
    "    for line in in_file:\n",
    "        key, value = line.split('\\t')\n",
    "        \n",
    "        if key != previous:\n",
    "            if previous is not None:\n",
    "                checkmax(previous, sum)\n",
    "            previous = key\n",
    "            sum = 0\n",
    "            \n",
    "        sum += int(value)\n",
    "        \n",
    "    checkmax(previous, sum)\n",
    "    in_file.close()\n",
    "except IOError:\n",
    "    print \"error performing file operation\"\n",
    "    \n",
    "print \"max: %s = %d\" % (M[0], M[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these three steps seem like a complicated way to achieve the same result, there are a few key differences:\n",
    "\n",
    "- In each of the three steps, the entire contents of the file never had to be held in memory. This means that the program is not affected by the same caching issues as the simple version.\n",
    "- The mapping function can be be split into many independent parallel tasks, each generating separate files. \n",
    "- The shuffing and reducing functions can also be split into many independent parallel tasks, with the final result being written to an output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the MapReduce algorithm can be parallelized easily and efficiently means that it is ideally suited for applications on very large data sets, as well as were resiliance is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce is clearly not a general-purpose framework for all forms of parallel programming. Rather, it is designed specifically for problems that can be broken up into the the map-reduce paradigm. Perhaps surprisingly, there are a lot of data analysis tasks that fit nicely into this model. While MapReduce is heavily used within Google, it also found use in companies such as Yahoo, Facebook, and Amazon.\n",
    "\n",
    "The original, and proprietary, implementation was done by Google. It is used internally for a large number of Google services. The Apache Hadoop project built a clone to specs defined by Google. Amazon, in turn, uses Hadoop MapReduce running on their EC2 (elastic cloud) computing-on-demand service to offer the Amazon Elastic MapReduce service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Apache Spark from [here](http://www.apache.org/dyn/closer.lua/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz). This should work with Windows, Linux, and Mac OS X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untar and uncompress the archive, then move it to a known location, e.g. `/home/user/spark`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command from a shell (where you would normally run `jupyter notebook`). This should start a notebook with spark enabled."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IPYTHON_OPTS=\"notebook\" /home/user/spark/bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark will automatically create a [`SparkContext`](https://spark.apache.org/docs/1.1.1/api/python/pyspark.context.SparkContext-class.html) for you to work with using the local Spark configuration. We can check that Spark is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x109064050>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming Spark applications is similar to other data flow languages that had previously been implemented on Hadoop:\n",
    "\n",
    "- A ***driver*** is code in a driver program which is lazily evaluated\n",
    "- One or more works, called ***executors***, run the driver code on their partitions of the RDD which is distributed across the cluster. \n",
    "- Results are then sent back to the driver for aggregation or compilation. \n",
    "\n",
    "Essentially the driver program creates one or more RDDs, applies operations to transform the RDD, then invokes some action on the transformed RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are outlined as follows:\n",
    "\n",
    "1. Define one or more RDDs either through accessing data stored on disk (HDFS, Cassandra, HBase, Local Disk), parallelizing some collection in memory, transforming an existing RDD, or by caching or saving.\n",
    "2. Invoke operations on the RDD by passing closures (functions) to each element of the RDD. Spark offers over 80 high level operators beyond Map and Reduce.\n",
    "3. Use the resulting RDDs with actions (e.g. count, collect, save, etc.). Actions kick off the computing on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Spark runs a closure on a worker, any variables used in the closure are copied to that node, but are maintained within the local scope of that closure. \n",
    "\n",
    "Spark provides two types of shared variables that can be interacted with by all workers in a restricted fashion. \n",
    "\n",
    "- ***Broadcast*** variables are distributed to all workers, but are read-only. These variables can be used as lookup tables or stopword lists. \n",
    "- ***Accumulators*** are variables that workers can \"add\" to using associative operations and are typically used as counters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, Spark applications are run as independent sets of processes, coordinated by a ***SparkContext*** in the driver program. The context will connect to some cluster manager (e.g. YARN) which allocates system resources. Each worker in the cluster is managed by an executor, which is in turn managed by the SparkContext. The executor manages computation as well as storage and caching on each machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important to note is that:\n",
    "- Application code is sent from the driver to the executors, and the executors specify the context and the various tasks to be run. \n",
    "- The executors communicate back and forth with the driver for data sharing or for interaction. \n",
    "- Drivers are key participants in Spark jobs, and therefore, they should be on the same network as the cluster. \n",
    "\n",
    "This is different from Hadoop code, where you might submit a job from anywhere to the JobTracker, which then handles the execution on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start using Spark, we have to create an [RDD](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html). The `SparkContext` provides a number of methods to do this. We will use the `textFile` method, which reads a file an creates an RDD of strings, one for each line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville', '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or', 're-use it under the terms of the Project Gutenberg License included', 'with this eBook or online at www.gutenberg.org', '', '', 'Title: Moby Dick; or The Whale', '']\n"
     ]
    }
   ],
   "source": [
    "text = sc.textFile('/tmp/pg2701.txt', use_unicode=False)\n",
    "print text.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same `splitter` function to split lines correctly. The `flatMap` method applies the function to all elements of the RDD and flattens the results into a single list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale']\n"
     ]
    }
   ],
   "source": [
    "words = text.flatMap(splitter)\n",
    "print words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the mapping step. This is simply the case of applying the function `lambda x: (x,1)` to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('project', 1), ('gutenberg', 1), ('ebook', 1), ('of', 1), ('moby', 1), ('dick', 1), ('or', 1), ('the', 1), ('whale', 1)]\n"
     ]
    }
   ],
   "source": [
    "words_mapped = words.map(lambda x: (x,1))\n",
    "print words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shuffling step is performed using the `sortByKey` methond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 1), ('', 1), ('', 1), ('', 1), ('', 1), ('', 1), ('', 1), ('', 1), ('', 1), ('', 1)]\n"
     ]
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "print sorted_map.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reduce step. The `reduceByKey` method uses the supplied function to merge values for each key. In this case, we use the `add` function to perform a sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 3235), ('unimaginative', 1), ('unscientific', 1), ('foul', 11), ('four', 74), ('gag', 2), ('prefix', 1), ('clotted', 2), ('plaudits', 1), ('looking', 70)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "counts = sorted_map.reduceByKey(add)\n",
    "print counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the `max` method to find the word with the maximum number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 14620)\n"
     ]
    }
   ],
   "source": [
    "print counts.max(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark also provides the `parallelize` method which distributes a local Python collection to form an RDD (obviously a cluster is required to obtain true parallelism.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how we can calculate the number of primes in a certain range of numbers. First, we define a function to check if a number is prime. This requires checking if it is divisible by all odd numbers up to the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an RDD comprising all numbers from 0 to `n` (in this case `n = 1000000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(xrange(1000000)) #creates a RDD using an iterative object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `filter` method to apply the function to each value, returning an RDD containing only values that evalute to `True`. We can then count these to determine the number of primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of primes in the RDD\n",
    "print nums.filter(isprime).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the methods available on the [SparkContext](https://spark.apache.org/docs/1.1.1/api/python/pyspark.context.SparkContext-class.html) and [RDD](https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html) objects (see links for details), write the following programs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Count the number of distinct words in the `pg2701.txt`.\n",
    "\n",
    "It's ok to reuse the `splitter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9cf249b9058253bc3d1b2c3ad4aa013b",
     "grade": true,
     "grade_id": "spark1",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17355\n"
     ]
    }
   ],
   "source": [
    "print words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute the product of all the numbers between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "04218449f991a0546e9c503c32085239",
     "grade": true,
     "grade_id": "spark2",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402387260077093773543702433923003985719374864210714632543799910429938512398629020592044208486969404800479988610197196058631666872994808558901323829669944590997424504087073759918823627727188732519779505950995276120874975462497043601418278094646496291056393887437886487337119181045825783647849977012476632889835955735432513185323958463075557409114262417474349347553428646576611667797396668820291207379143853719588249808126867838374559731746136085379534524221586593201928090878297308431392844403281231558611036976801357304216168747609675871348312025478589320767169132448426236131412508780208000261683151027341827977704784635868170164365024153691398281264810213092761244896359928705114964975419909342221566832572080821333186116811553615836546984046708975602900950537616475847728421889679646244945160765353408198901385442487984959953319101723355556602139450399736280750137837615307127761926849034352625200015888535147331611702103968175921510907788019393178114194545257223865541461062892187960223838971476088506276862967146674697562911234082439208160153780889893964518263243671616762179168909779911903754031274622289988005195444414282012187361745992642956581746628302955570299024324153181617210465832036786906117260158783520751516284225540265170483304226143974286933061690897968482590125458327168226458066526769958652682272807075781391858178889652208164348344825993266043367660176999612831860788386150279465955131156552036093988180612138558600301435694527224206344631797460594682573103790084024432438465657245014402821885252470935190620929023136493273497565513958720559654228749774011413346962715422845862377387538230483865688976461927383814900140767310446640259899490222221765904339901886018566526485061799702356193897017860040811889729918311021171229845901641921068884387121855646124960798722908519296819372388642614839657382291123125024186649353143970137428531926649875337218940694281434118520158014123344828015051399694290153483077644569099073152433278288269864602789864321139083506217095002597389863554277196742822248757586765752344220207573630569498825087968928162753848863396909959826280956121450994871701244516461260379029309120889086942028510640182154399457156805941872748998094254742173582401063677404595741785160829230135358081840096996372524230560855903700624271243416909004153690105933983835777939410970027753472000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "from operator import mul\n",
    "numset = sc.parallelize(xrange(1, 1001))\n",
    "print numset.fold(1,mul) # fold(0 elem in the algebra, op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using the MapReduce approach, calculate the average of the square root of all the numbers between 1 and 1000.\n",
    "\n",
    "Hint: Use the `map` and `fold` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d91ffc6138be788b2bd994fb51cf08ce",
     "grade": true,
     "grade_id": "spark3",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0974558875\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "n = numset.count()\n",
    "sq_numset = numset.map(lambda x: x**0.5/n)\n",
    "print sq_numset.fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Benjamin Bengfort, [Getting Started with Spark (in Python)](https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python)\n",
    "- [A Hands-on Introduction to MapReduce in Python](https://zettadatanet.wordpress.com/2015/04/04/a-hands-on-introduction-to-mapreduce-in-python)\n",
    "- Lucas Allen, [Spark Dataframes and MLlib](http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
