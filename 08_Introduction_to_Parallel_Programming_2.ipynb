{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA-3001 Advanced Python for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure you **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart). You can then run the cells **in order**, during the class.\n",
    "\n",
    "Any textual answers that need to be provided will be marked with \"YOUR ANSWER HERE\". Replace this text with your answer to the question.\n",
    "\n",
    "Any code answers that need to be provided will be marked with:\n",
    "\n",
    "```\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "Replace all this code with your answer to the question. If you do not answer the question, the `NotImplementedError` exception will be raised, which will indicate to the grader that no answer has been supplied.\n",
    "\n",
    "In many cases, code answers will also have some associated test code. You should execute the tests after you have entered your code in order to ensure that your answer is correct. You should not proceed to the next question until your answer is correct.\n",
    "\n",
    "Finally, insert your Net ID and the Net ID's of any collaborators in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "NET_ID = \"jl6583\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Parallel Programming with MPI (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed you have set up for MPI (either Docker or non-Docker) using the instructions from the first part. However, you will probably need to intialize your environment prior to starting this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the MPI cluster\n",
    "\n",
    "Check that the **IPython Clusters** tab is visible. Switch to this tab and check and there is an **mpi** entry on the tab. Press the refresh button if not:\n",
    "\n",
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgamVKb1cyU210S0k)\n",
    "\n",
    "On the **mpi** entry, set the **# of engines** to **4**, then click the **Start** button. \n",
    "\n",
    "***Note that the # of engines must be less than or equal to the number of engines started.***\n",
    "\n",
    "### 2. Initialize MPI\n",
    "\n",
    "Run the following code to initialize MPI. Make sure there are no errors. \n",
    "\n",
    "*** Note: You may need to wait a few seconds after starting the engines before this code will work. Try running it again if you get an error.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from ipyparallel import Client, error\n",
    "cluster = Client(profile=\"mpi\")\n",
    "view = cluster[:]\n",
    "view.block=True\n",
    "print len(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non blocking communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how to send and receive messages using blocking communication. In this case, the sender or receiver is not able to perform any other actions until the corresponding message has been sent or received (to be accurate, it is actually until the buffer is safe to use.)\n",
    "\n",
    "Blocking communication has a number of disadvantages. Potential computational time is simply wasted while waiting for the call to complete. And as we have seen, blocking communication can also lead to deadlock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate approach is to allow the program to continue execution while the messages is being sent or received. This is known as non blocking communcation, and in MPI, is achieved using the `Isend` and `Irecv` methods. These methods return immediately, however the buffer is *not safe* for reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Isend` and `Irecv` methods initiate a send and receive operation respectively. These methods return a instance of the `Request` class, which uniquely identifys the started operation. The completion can then be managed using the `Test`, `Wait`, and `Cancel` methods of the `Request` class. The management of `Request` objects and associated memory buffers involved in communication requires careful coordination. Users must ensure that objects exposing their memory buffers are not accessed at the Python level while they are involved in nonblocking message-passing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example performs the same simple send and receive as demonstrated previously, however this time it is done with the non blocking versions of the send and receive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "If you commented out the `Isend` and both `Wait` calls you would see that this code will not deadlock (if you don't comment out the `Wait` calls, then you have effectively the same code as before and it ***will*** deadlock.) However, unless you call `Cancel`, the Python kernel will eventually deadlock anyway as there will be an unequal number of messages posted, so I don't recommend doing it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "randNum = numpy.zeros(1)\n",
    "\n",
    "\n",
    "if rank == 1:\n",
    "        randNum = numpy.random.random_sample(1)\n",
    "        print \"Process\", rank, \"drew the number\", randNum[0]\n",
    "        req = comm.Isend(randNum, dest=0)\n",
    "        req.Wait()\n",
    "\n",
    "if rank == 0:\n",
    "        print \"Process\", rank, \"before receiving has the number\", randNum[0]\n",
    "        req = comm.Irecv(randNum, source=1)\n",
    "        req.Wait()\n",
    "        print \"Process\", rank, \"received the number\", randNum[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a non blocking version of the send and receive program. Note there is no need to wait after process 1 sends the message, nor after process 0 sends the reply. However it ***is*** necessary for process 1 to wait for the reply so that it knows the message has been fully received before trying to print it out. Similarly, process 0 must wait for the full message before trying to compute `randNum * 2`. Run it to verify the program works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Process 1 drew the number 0.331456767838\n",
      "Process 1 received the number 0.662913535677\n",
      "[stdout:1] \n",
      "Process 0 before receiving has the number 0.0\n",
      "Process 0 received the number 0.331456767838\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "randNum = numpy.zeros(1)\n",
    "diffNum = numpy.random.random_sample(1)\n",
    "\n",
    "if rank == 1:\n",
    "        randNum = numpy.random.random_sample(1)\n",
    "        print \"Process\", rank, \"drew the number\", randNum[0]\n",
    "        comm.Isend(randNum, dest=0)\n",
    "        req = comm.Irecv(randNum, source=0)\n",
    "        req.Wait()\n",
    "        print \"Process\", rank, \"received the number\", randNum[0]\n",
    "\n",
    "if rank == 0:\n",
    "        print \"Process\", rank, \"before receiving has the number\", randNum[0]\n",
    "        req = comm.Irecv(randNum, source=1)\n",
    "        req.Wait()\n",
    "        print \"Process\", rank, \"received the number\", randNum[0]\n",
    "        randNum *= 2\n",
    "        comm.Isend(randNum, dest=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Modify this program so that process 1 overlaps a computation with sending the message and receiving the reply. The computation should be dividing diffNum by 3.14 and printing the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "606b90182c50e2c4c9de7b0838f31346",
     "grade": true,
     "grade_id": "par1",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Process 1 drew the number 0.934555320787\n",
      "Process 1 has diffNum 0.297629083053\n",
      "Process 1 received the number 1.86911064157\n",
      "[stdout:1] \n",
      "Process 0 before receiving has the number 0.0\n",
      "Process 0 received the number 0.934555320787\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "randNum = numpy.zeros(1)\n",
    "diffNum = numpy.random.random_sample(1)\n",
    "\n",
    "if rank == 1:\n",
    "        randNum = numpy.random.random_sample(1)\n",
    "        diffNum = randNum[0]\n",
    "        print \"Process\", rank, \"drew the number\", randNum[0]\n",
    "        comm.Isend(randNum, dest=0)\n",
    "        diffNum /= 3.14\n",
    "        print 'Process', rank, 'has diffNum', diffNum\n",
    "        req = comm.Irecv(randNum, source=0)\n",
    "        req.Wait()\n",
    "        print \"Process\", rank, \"received the number\", randNum[0]\n",
    "\n",
    "if rank == 0:\n",
    "        print \"Process\", rank, \"before receiving has the number\", randNum[0]\n",
    "        req = comm.Irecv(randNum, source=1)\n",
    "        req.Wait()\n",
    "        print \"Process\", rank, \"received the number\", randNum[0]\n",
    "        randNum *= 2\n",
    "        comm.Isend(randNum, dest=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to test without waiting using `Request.Test()`. This method will return `True` when the message operation has completed. To cancel a pending communication, call `Request.Cancel()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first steps in designing a parallel program is to break the problem into discrete \"chunks\" of work that can be distributed to multiple tasks. This is known as decomposition or partitioning. There are two main ways to decompose an algorithm: *domain decomposition* and *functional decomposition*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of partitioning, the data associated with a problem is decomposed. Each parallel task then works on a portion of the data.\n",
    "\n",
    "* Data divided into pieces of same size and mapped to different processors\n",
    "* Processor works only on data assigned to it\n",
    "* Communicates with other processors when necessary\n",
    "* Examples of domain (data) decomposition\n",
    "  * Embarrassingly parallel applications (Monte Carlo simulations)\n",
    "  * Finite difference calculations\n",
    "  * Numerical integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgQ0FBWnFfVTRlbnM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to partition the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgRm04SnZ0WFlvbjA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional decomposition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus is on the computation that is to be performed rather than on the data manipulated by the computation. The problem is decomposed according to the work that must be done. Each task then performs a portion of the overall work.\n",
    "\n",
    "* Used when pieces of data require different processing times\n",
    "* Performance limited by the slowest process\n",
    "* Program decomposed into a number of small tasks\n",
    "* Tasks assigned to processors as they become available\n",
    "* Implemented in a master/slave paradigm\n",
    "* Examples of functional decomposition\n",
    "  * Surface reconstruction from a finite element mesh\n",
    "  * Searching images or data bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgemhaYzJOVTVJZVk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain decomposition example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of domain decomposition can be seen by computing a simple integral using the Mid-point rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate cos(x) by Mid-point Rule\n",
    "\n",
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgdl9SNnNyZ3dUWjg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle\\int_{a}^{b}cos(x)dx = \\displaystyle\\sum_{i=0}^{p-1}\\displaystyle\\sum_{j=0}^{n-1}\\int_{a_i+j*h}^{a_i+(j+1)*h} cos(x)dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\approx \\displaystyle\\sum_{i=0}^{p-1}\\left[\\displaystyle\\sum_{j=0}^{n-1}cos(a_{ij})*h\\right]$ where $\\begin{cases}h=\\dfrac{b-a}{pn}\\\\a_i=a+i*n*h\\\\a_{ij}=a_i+(j+0.5)*h\\end{cases}$\n",
    "\n",
    "$p$ is the number of partitions\n",
    "\n",
    "$n$ is the number of increments per partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the serial code to implement this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral =  1.0000000257\n"
     ]
    }
   ],
   "source": [
    "from math import acos, cos\n",
    "\n",
    "# Compute the inner sum\n",
    "def integral(ai, h, n):\n",
    "    integ = 0.0\n",
    "    for j in range(n):\n",
    "        aij = ai + (j + 0.5) * h\n",
    "        integ += cos(aij) * h\n",
    "    return integ\n",
    "\n",
    "pi = 3.14159265359\n",
    "p = 4\n",
    "n = 500\n",
    "a = 0.0\n",
    "b = pi / 2.0\n",
    "h = (b - a) / (n * p)\n",
    "\n",
    "integral_sum = 0.0\n",
    "\n",
    "# Compute the outer sum\n",
    "for i in range(p):\n",
    "    ai = a + i * n * h\n",
    "    integral_sum += integral(ai, h, n)\n",
    "    \n",
    "print \"The integral = \", integral_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel point-to-point version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the problem has been decomposed into separate partitions, it is easy to implement a parallel version of the algorithm. In this case, each of the partitions can be computed by a separate process. Once each process has computed its partition, it sends the result back to a root process (in this case process 0) which sums the values and prints the final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Process  1  has the partial integral  0.32442335716\n",
      "[stdout:1] \n",
      "Process  0  has the partial integral  0.382683442201\n",
      "The integral =  1.0000000257\n",
      "[stdout:2] Process  2  has the partial integral  0.216772756896\n",
      "[stdout:3] Process  3  has the partial integral  0.0761204694451\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from math import acos, cos\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "def integral(ai, h, n):\n",
    "    integ = 0.0\n",
    "    for j in range(n):\n",
    "        aij = ai + (j + 0.5) * h\n",
    "        integ += cos(aij) * h\n",
    "    return integ\n",
    "\n",
    "pi = 3.14159265359\n",
    "n = 500\n",
    "a = 0.0\n",
    "b = pi / 2.0\n",
    "h = (b - a) / (n * size)\n",
    "ai = a + rank * n * h\n",
    "\n",
    "# All processes initialize my_int with their partition calculation\n",
    "my_int = numpy.full(1, integral(ai, h, n))\n",
    "\n",
    "print \"Process \", rank, \" has the partial integral \", my_int[0]\n",
    "\n",
    "if rank == 0:\n",
    "    # Process 0 receives all the partitions and computes the sum\n",
    "    integral_sum = my_int[0]\n",
    "    for p in range(1, size):\n",
    "        comm.Recv(my_int, source=p)\n",
    "        integral_sum += my_int[0]\n",
    "    \n",
    "    print \"The integral = \", integral_sum\n",
    "else:\n",
    "    # All other processes just send their partition values to process 0\n",
    "    comm.Send(my_int, dest=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many situations in parallel programming when groups of processes need to exchange messages. Rather than explicitly sending and receiving such messages, MPI provides group operations known as collectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collective communications allow the sending of data between multiple processes of a group simultaneously. Collective functions come in blocking and non-blocking versions.\n",
    "\n",
    "The more commonly used collective communication operations are the following:\n",
    "\n",
    "* Synchronization\n",
    "  * Processes wait until all members of the group have reached the synchronization point\n",
    "* Global communication functions\n",
    "  * Broadcast data from one member to all members of a group\n",
    "  * Gather data from all members to one member of a group\n",
    "  * Scatter data from one member to all members of a group\n",
    "* Collective computation (reductions)\n",
    "  * One member of the group collects data from the other members and performs an operation (min, max, add, multiply, etc.) on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=0B_3lImS7uRMgY1RuYTNtNkFMbW8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Collective communication routines must involve all processes within the scope of a communicator.***\n",
    "\n",
    "All processes are by default, members in the communicator MPI.COMM_WORLD, however additional communicators can be defined by the programmer (beyond the scope of this course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Unexpected behavior, including program failure, can occur if even one task in the communicator doesn't participate.\n",
    "It is the programmer's responsibility to ensure that all processes within a communicator participate in any collective operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example collective operations\n",
    "\n",
    "#### `Comm.Barrier()`\n",
    "Synchronization operation. Creates a barrier synchronization in a group. Each task, when reaching the `Barrier()` call, blocks until all tasks in the group reach a `Barrier()` call. Then all tasks are free to proceed.\n",
    "\n",
    "#### `Comm.Bcast(buf, root=0)`\n",
    "Data movement operation. Broadcasts (sends) a message from the process with rank \"root\" to all other processes in the group. \n",
    "\n",
    "#### `Comm.Scatter(sendbuf, recvbuf, root=0)`\n",
    "Data movement operation. Distributes distinct messages from a single source task to each task in the group. \n",
    "\n",
    "#### `Comm.Gather(sendbuf, recvbuf, root=0)`\n",
    "Data movement operation. Gathers distinct messages from each task in the group to a single destination task. This routine is the reverse operation of `Scatter()`. \n",
    "\n",
    "#### `Comm.Alltoall(sendbuf, recvbuf)`\n",
    "All-to-all Scatter/Gather, send data from all to all processes in a group.\n",
    "\n",
    "#### `Comm.Reduce(sendbuf, recvbuf, op=MPI.SUM, root=0)`\n",
    "Reduces values on all processes to a single value by applying the operation `op`. Operations include:\n",
    "* `MPI.MAX` - Returns the maximum element.\n",
    "* `MPI.MIN` - Returns the minimum element.\n",
    "* `MPI.SUM` - Sums the elements.\n",
    "* `MPI.PROD` - Multiplies all elements.\n",
    "* `MPI.LAND` - Performs a logical and across the elements.\n",
    "* `MPI.LOR` - Performs a logical or across the elements.\n",
    "* `MPI.BAND` - Performs a bitwise and across the bits of the elements.\n",
    "* `MPI.BOR` - Performs a bitwise or across the bits of the elements.\n",
    "* `MPI.MAXLOC` - Returns the maximum value and the rank of the process that owns it.\n",
    "* `MPI.MINLOC` - Returns the minimum value and the rank of the process that owns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel collective version of Mid-point rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows how the mid-point rule can be computed using collective operations. \n",
    "\n",
    "We choose to broadcast the number of increments per partition `n` to each process, although this is not strictly necessary. Once the processes have received `n` they are able to compute their partition. The processes then send the values back to the `root` process using `Reduce` which automatically computes the sum of all the values and places the result in `integral_sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Process  1  before n =  0.0\n",
      "Process  1  after n =  500.0\n",
      "Process  1  has the partial integral  0.32442335716\n",
      "[stdout:1] \n",
      "Process  0  before n =  500.0\n",
      "Process  0  after n =  500.0\n",
      "Process  0  has the partial integral  0.382683442201\n",
      "The Integral Sum = 1.0000000257\n",
      "[stdout:2] \n",
      "Process  2  before n =  0.0\n",
      "Process  2  after n =  500.0\n",
      "Process  2  has the partial integral  0.216772756896\n",
      "[stdout:3] \n",
      "Process  3  before n =  0.0\n",
      "Process  3  after n =  500.0\n",
      "Process  3  has the partial integral  0.0761204694451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:1] \n",
      "/Users/christopher/anaconda/lib/python2.7/site-packages/numpy/core/numeric.py:294: FutureWarning: in the future, full(1, 500) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from math import acos, cos\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "def integral(ai, h, n):\n",
    "    integ = 0.0\n",
    "    for j in range(n):\n",
    "        aij = ai + (j + 0.5) * h\n",
    "        integ += cos(aij) * h\n",
    "    return integ\n",
    "\n",
    "pi = 3.14159265359\n",
    "a = 0.0\n",
    "b = pi / 2.0\n",
    "h = (b - a) / (n * size)\n",
    "dest = 0\n",
    "n = numpy.zeros(1)\n",
    "my_int = numpy.zeros(1)\n",
    "integral_sum = numpy.zeros(1)\n",
    "\n",
    "# Initialize value of n only if this is rank 0\n",
    "if rank == 0:\n",
    "    n = numpy.full(1, 500) # default value\n",
    "    \n",
    "# Broadcast n to all processes\n",
    "print \"Process \", rank, \" before n = \", n[0]\n",
    "comm.Bcast(n, root=0)\n",
    "print \"Process \", rank, \" after n = \", n[0]\n",
    "\n",
    "# Compute partition\n",
    "ai = a + rank * h * n\n",
    "my_int[0] = integral(ai, h ,n)\n",
    "\n",
    "# Send partition back to root process, computing sum across all partitions\n",
    "print \"Process \", rank, \" has the partial integral \", my_int[0]\n",
    "comm.Reduce(my_int, integral_sum, MPI.SUM, dest)\n",
    "\n",
    "# Only print the result in process 0\n",
    "if rank == 0:\n",
    "    print 'The Integral Sum =', integral_sum[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Modify the above code to broadcast both the number of increments `n` *and* the increment width `h` to each process. Hint: `h` will need to be a NumPy array.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7b7328c4ebf347152e850569139a8c83",
     "grade": true,
     "grade_id": "par2",
     "locked": false,
     "points": 4,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Process  1  before n =  0.0  h =  0.0\n",
      "Process  1  after n =  500.0  h =  0.000785398163398\n",
      "Process  1  has the partial integral  0.32442335716\n",
      "[stdout:1] \n",
      "Process  0  before n =  500.0  h =  0.000785398163398\n",
      "Process  0  after n =  500.0  h =  0.000785398163398\n",
      "Process  0  has the partial integral  0.382683442201\n",
      "The Integral Sum = 1.0000000257\n",
      "[stdout:2] \n",
      "Process  2  before n =  0.0  h =  0.0\n",
      "Process  2  after n =  500.0  h =  0.000785398163398\n",
      "Process  2  has the partial integral  0.216772756896\n",
      "[stdout:3] \n",
      "Process  3  before n =  0.0  h =  0.0\n",
      "Process  3  after n =  500.0  h =  0.000785398163398\n",
      "Process  3  has the partial integral  0.0761204694451\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import numpy\n",
    "from math import acos, cos\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "def integral(ai, h, n):\n",
    "    integ = 0.0\n",
    "    for j in range(n):\n",
    "        aij = ai + (j + 0.5) * h\n",
    "        integ += cos(aij) * h\n",
    "    return integ\n",
    "\n",
    "pi = 3.14159265359\n",
    "a = 0.0\n",
    "b = pi / 2.0\n",
    "h = numpy.zeros(1)\n",
    "dest = 0\n",
    "n = numpy.zeros(1)\n",
    "my_int = numpy.zeros(1)\n",
    "integral_sum = numpy.zeros(1)\n",
    "\n",
    "# Initialize value of n only if this is rank 0\n",
    "if rank == 0:\n",
    "    n = numpy.full(1, 500) # default value\n",
    "    h = numpy.full(1,(b - a) / (n * size))\n",
    "    \n",
    "# Broadcast n to all processes\n",
    "print \"Process \", rank, \" before n = \", n[0], \" h = \", h[0]\n",
    "comm.Bcast(n, root=0)\n",
    "comm.Bcast(h, root=0)\n",
    "print \"Process \", rank, \" after n = \", n[0], \" h = \", h[0]\n",
    "\n",
    "# Compute partition\n",
    "ai = a + rank * h * n\n",
    "my_int[0] = integral(ai, h ,n)\n",
    "\n",
    "# Send partition back to root process, computing sum across all partitions\n",
    "print \"Process \", rank, \" has the partial integral \", my_int[0]\n",
    "comm.Reduce(my_int, integral_sum, MPI.SUM, dest)\n",
    "\n",
    "# Only print the result in process 0\n",
    "if rank == 0:\n",
    "    print 'The Integral Sum =', integral_sum[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpi4py supports two kinds of Python data objects. When using the upper case version of the methods (`Send`, `Irecv`, `Gather`, etc.) the data object must support the single-segment buffer interface. This interface is a standard Python mechanism provided by some types (e.g., strings and numeric arrays), which is why we have been using NumPy arrays in the examples. It is also possible to transmit an arbitrary Python data type using the lower case version of the methods (`send`, `irecv`, `gather`, etc.) mpi4py will serialize the data type, send it to the remote process, then deserialize it back to the original data type (a process known as *pickling* and *unpickling*). While this is simple, it also adds significant overhead to the MPI operaton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other MPI operations available that we have not touched on here. Please refer to the [mpi4py documentation](http://mpi4py.readthedocs.org/en/stable/index.html) for more information if you are interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
